#My actual data in which I am going to perform is kept in local file system. So, here we simply upload are data into workspace of iNeuron Lab.
#And from there I upload this same data to HDFS. 
#I mentioned the steps which is used to do so.

#Step 1:-  Navigate to the file which you want to upload and simply drag and drop into ineuron lab work space.
#step 2:-  Upload this data from workspace to the HDFS, for that write the below command in terminal. 
    # 2.1 First you have to create one directory.
     >>> hadoop fs -mkdir /HadoopNewFile
     or
     >>> hdfs dfs -mkdir /HadoopNewFile
    # 2.2 To confirm whether the directory is created or not, write the below command
     >>> hadoop fs -ls /
     or 
     >>> hdfs dfs -ls /
    # 2.3 Now it time to upload the data file into HDFS
     >>> hadoop fs -put <workspace-path> /<HDFS-path>
     or
     >>>hdfs dfs -put <workspace-path> /<HDFS-path>
     
     #In my case, Example:
      >>> hdfs dfs -put Flat_Data.csv /HadoopNewFile
     
#Now, it's time to perform some operations on the data. For that first you have to lunch new terminal in the same workspace of Neuro Lab.
#and then write the below in the terminal.
>>> pyspark 

#you will see the below result after pressing the enter⤴️ buttom 

Python 3.9.13 (main, Oct 13 2022, 21:15:33) 
[GCC 11.2.0] :: Anaconda, Inc. on linux
Type "help", "copyright", "credits" or "license" for more information.
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
23/03/26 13:12:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.2.1
      /_/

Using Python version 3.9.13 (main, Oct 13 2022 21:15:33)
Spark context Web UI available at http://40f1a1d4b7dd:4040
Spark context available as 'sc' (master = local[*], app id = local-1679816529162).
SparkSession available as 'spark'.

#Read the data from the HDFS 
>>> df = spark.read.option("header",True).option("inferSchema",True).csv("/HadoopFile/Flat_Data.csv")
>>> df.show(5) # Here I limit the no.of row by specifying the 5 rows from the top.
+---------+-------------+--------------------+------------------+---------+--------+
|UniqueID |      LandUse|     PropertyAddress|          SaleDate|SalePrice|Bedrooms|
+---------+-------------+--------------------+------------------+---------+--------+
|     2045|SINGLE FAMILY|1808  FOX CHASE D...|     April 9, 2013|   240000|       3|
|    16918|SINGLE FAMILY|1832  FOX CHASE D...|     June 10, 2014|   366000|       3|
|    54582|SINGLE FAMILY|1864 FOX CHASE  D...|September 26, 2016|   435000|       4|
|    43070|SINGLE FAMILY|1853  FOX CHASE D...|  January 29, 2016|   255000|       3|
|    22714|SINGLE FAMILY|1829  FOX CHASE D...|  October 10, 2014|   278000|       4|
+---------+-------------+--------------------+------------------+---------+--------+
only showing top 5 rows

# Now it's time to get your hand dirty by performing some operations as below.

>>> df.select("*").show()

+---------+-------------+--------------------+------------------+---------+--------+
|UniqueID |      LandUse|     PropertyAddress|          SaleDate|SalePrice|Bedrooms|
+---------+-------------+--------------------+------------------+---------+--------+
|     2045|SINGLE FAMILY|1808  FOX CHASE D...|     April 9, 2013|   240000|       3|
|    16918|SINGLE FAMILY|1832  FOX CHASE D...|     June 10, 2014|   366000|       3|
|    54582|SINGLE FAMILY|1864 FOX CHASE  D...|September 26, 2016|   435000|       4|
|    43070|SINGLE FAMILY|1853  FOX CHASE D...|  January 29, 2016|   255000|       3|
|    22714|SINGLE FAMILY|1829  FOX CHASE D...|  October 10, 2014|   278000|       4|
|    18367|SINGLE FAMILY|1821  FOX CHASE D...|     July 16, 2014|   267000|       3|
|    19804|SINGLE FAMILY|2005  SADIE LN, G...|   August 28, 2014|   171000|       3|
|    54583|SINGLE FAMILY|1917 GRACELAND  D...|September 27, 2016|   262000|       3|
|    36500|SINGLE FAMILY|1428  SPRINGFIELD...|   August 14, 2015|   285000|       3|
|    19805|SINGLE FAMILY|1420  SPRINGFIELD...|   August 29, 2014|   340000|       5|
+---------+-------------+--------------------+------------------+---------+--------+
only showing top 10 rows

# we can also specify the columns inside the select function.
>>> df.select("LandUse","SaleDate","SalePrice").show()

+-------------+------------------+---------+
|      LandUse|          SaleDate|SalePrice|
+-------------+------------------+---------+
|SINGLE FAMILY|     April 9, 2013|   240000|
|SINGLE FAMILY|     June 10, 2014|   366000|
|SINGLE FAMILY|September 26, 2016|   435000|
|SINGLE FAMILY|  January 29, 2016|   255000|
|SINGLE FAMILY|  October 10, 2014|   278000|
|SINGLE FAMILY|     July 16, 2014|   267000|
|SINGLE FAMILY|   August 28, 2014|   171000|
|SINGLE FAMILY|September 27, 2016|   262000|
|SINGLE FAMILY|   August 14, 2015|   285000|
|SINGLE FAMILY|   August 29, 2014|   340000|
+-------------+------------------+---------+
only showing top 10 rows

# we can also see the no. of rows in the data
>>> df.count()
56480

# we can also filter our data using where function as in the SQL we use WHERE Clause.

>>> df.select("*").where("Bedrooms = 3").show(10)
+---------+-------------+--------------------+------------------+---------+--------+
|UniqueID |      LandUse|     PropertyAddress|          SaleDate|SalePrice|Bedrooms|
+---------+-------------+--------------------+------------------+---------+--------+
|     2045|SINGLE FAMILY|1808  FOX CHASE D...|     April 9, 2013|   240000|       3|
|    16918|SINGLE FAMILY|1832  FOX CHASE D...|     June 10, 2014|   366000|       3|
|    43070|SINGLE FAMILY|1853  FOX CHASE D...|  January 29, 2016|   255000|       3|
|    18367|SINGLE FAMILY|1821  FOX CHASE D...|     July 16, 2014|   267000|       3|
|    19804|SINGLE FAMILY|2005  SADIE LN, G...|   August 28, 2014|   171000|       3|
|    54583|SINGLE FAMILY|1917 GRACELAND  D...|September 27, 2016|   262000|       3|
|    36500|SINGLE FAMILY|1428  SPRINGFIELD...|   August 14, 2015|   285000|       3|
|    16920|SINGLE FAMILY|637  GAYLEMORE DR...|     June 30, 2014|   247400|       3|
|    28155|SINGLE FAMILY|644  GAYLEMORE DR...|    March 31, 2015|   185900|       3|
|     8899|SINGLE FAMILY|1921  NORMERLE DR...|  October 11, 2013|   349900|       3|
+---------+-------------+--------------------+------------------+---------+--------+
only showing top 10 rows

# Create temporary view and write SQL like query in the Pyspark
>>> df.createOrReplaceTempView("TAB")
>>> spark.sql("SELECT DISTINCT * FROM TAB").show()
+---------+-------------+--------------------+------------------+---------+--------+
|UniqueID |      LandUse|     PropertyAddress|          SaleDate|SalePrice|Bedrooms|
+---------+-------------+--------------------+------------------+---------+--------+
|     2045|SINGLE FAMILY|1808  FOX CHASE D...|     April 9, 2013|   240000|       3|
|    16918|SINGLE FAMILY|1832  FOX CHASE D...|     June 10, 2014|   366000|       3|
|    54582|SINGLE FAMILY|1864 FOX CHASE  D...|September 26, 2016|   435000|       4|
|    43070|SINGLE FAMILY|1853  FOX CHASE D...|  January 29, 2016|   255000|       3|
|    22714|SINGLE FAMILY|1829  FOX CHASE D...|  October 10, 2014|   278000|       4|
|    18367|SINGLE FAMILY|1821  FOX CHASE D...|     July 16, 2014|   267000|       3|
|    19804|SINGLE FAMILY|2005  SADIE LN, G...|   August 28, 2014|   171000|       3|
|    54583|SINGLE FAMILY|1917 GRACELAND  D...|September 27, 2016|   262000|       3|
|    36500|SINGLE FAMILY|1428  SPRINGFIELD...|   August 14, 2015|   285000|       3|
|    19805|SINGLE FAMILY|1420  SPRINGFIELD...|   August 29, 2014|   340000|       5|
+---------+-------------+--------------------+------------------+---------+--------+
only showing top 10 rows
>>> spark.sql("SELECT LandUse, SaleDate, Bedrooms FROM TAB").show(10)
+-------------+------------------+--------+
|      LandUse|          SaleDate|Bedrooms|
+-------------+------------------+--------+
|SINGLE FAMILY|     April 9, 2013|       3|
|SINGLE FAMILY|     June 10, 2014|       3|
|SINGLE FAMILY|September 26, 2016|       4|
|SINGLE FAMILY|  January 29, 2016|       3|
|SINGLE FAMILY|  October 10, 2014|       4|
|SINGLE FAMILY|     July 16, 2014|       3|
|SINGLE FAMILY|   August 28, 2014|       3|
|SINGLE FAMILY|September 27, 2016|       3|
|SINGLE FAMILY|   August 14, 2015|       3|
|SINGLE FAMILY|   August 29, 2014|       5|
+-------------+------------------+--------+
only showing top 10 rows

>>> spark.sql("Select Distinct Bedrooms From TAB").show(10)
+--------+                                                                      
|Bedrooms|
+--------+
|    null|
|       1|
|       6|
|       3|
|       5|
|       9|
|       4|
|       8|
|       7|
|      10|
+--------+
only showing top 10 rows

>>> spark.sql("SELECT Distinct Bedrooms From TAB").count()
13

#In the similar way we can also perform more operations over the Data.
